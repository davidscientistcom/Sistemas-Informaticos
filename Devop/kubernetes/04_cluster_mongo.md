# Cluster de MongoDB: Fundamentos, Arquitectura y Operaci√≥n

##  Objetivo

Este documento explica **c√≥mo funciona un cl√∫ster de MongoDB**, qu√© componentes tiene, c√≥mo se configura y qu√© aspectos hay que tener en cuenta para garantizar la alta disponibilidad y consistencia de los datos.

No trabajaremos a√∫n con Kubernetes. Este artefacto sirve para entender perfectamente **qu√© requisitos debe cumplir MongoDB para funcionar en modo cl√∫ster**, y c√≥mo responder ante fallos, elecciones de nodo primario, y replicaci√≥n de datos.



##  ¬øQu√© es un cl√∫ster de MongoDB?

Un cl√∫ster de MongoDB es una **infraestructura distribuida** que permite:

* Alta disponibilidad
* Escalado horizontal o vertical
* Tolerancia a fallos
* Reparto o replicaci√≥n de los datos

MongoDB tiene dos mecanismos principales:

1. **Replica Set**: Mismo conjunto de datos replicado entre nodos
2. **Sharding**: Reparto de datos entre varios shards (para volumen masivo)

Nos centraremos en **Replica Set**, que es la base para cualquier sistema tolerante a fallos, pero tambi√©n introduciremos **sharding** como modelo de escalado horizontal.



##  1. Arquitectura de un Replica Set

Un Replica Set est√° formado por:

* üîë **Primary**: nodo que recibe escrituras y lecturas (por defecto)
* üîç **Secondary**: replica el contenido del primary; puede aceptar lecturas si se configura
* üö´ **Arbiter** (opcional): no almacena datos, pero ayuda a desempatar elecciones

Se recomienda tener un n√∫mero impar de nodos (3, 5, 7...) para garantizar **qu√≥rum**.



##  2. Mecanismo de Elecci√≥n y Qu√≥rum

### ¬øQu√© es el qu√≥rum?

En sistemas distribuidos como MongoDB, el qu√≥rum es el n√∫mero m√≠nimo de nodos que deben estar de acuerdo para tomar decisiones, como **elegir un nuevo nodo primario**. MongoDB usa un protocolo de consenso basado en votaci√≥n.

* Cada nodo tiene un voto (m√°ximo 7 votos por Replica Set)
* Para elegir un nuevo primary, se necesita m√°s de la mitad de los votos disponibles

**Ejemplo:**

* 3 nodos ‚Üí qu√≥rum m√≠nimo: 2
* 5 nodos ‚Üí qu√≥rum m√≠nimo: 3

### ¬øQu√© pasa si el nodo primary cae?

1. Los nodos restantes detectan la p√©rdida mediante heartbeats (cada 2 segundos por defecto)
2. Inician una **elecci√≥n** (election)
3. Gana el nodo con:

   * Datos m√°s actualizados (mayor `optime`)
   * Mayor prioridad (si se ha configurado)

Si no hay qu√≥rum:

* El cl√∫ster entra en **modo de solo lectura** (no hay primary)
* Se evita la inconsistencia de datos al evitar escrituras simult√°neas



##  3. Sharding: Escalado Horizontal

### ¬øQu√© es sharding?

Sharding es la t√©cnica de **dividir los datos entre m√∫ltiples cl√∫steres o nodos** para permitir manejar vol√∫menes masivos de datos y distribuir la carga.

Un cl√∫ster shardeado en MongoDB est√° compuesto por:

* **Shards**: Replica sets que contienen fragmentos de los datos
* **Config Servers**: Guardan metadatos y mapeo de qu√© shard tiene cada dato
* **Mongos**: Router que recibe las peticiones del cliente y las dirige al shard adecuado

### ¬øC√≥mo se decide d√≥nde va cada dato?

Los datos se dividen en **rangos o hashes** seg√∫n una **clave shard**. Por ejemplo, si la clave es `cliente_id`, Mongo puede guardar los IDs 1‚Äì1000 en el shard A, 1001‚Äì2000 en el shard B, etc.

### ¬øY si un shard se cae?

Como cada shard es un Replica Set, **mantiene alta disponibilidad** por s√≠ mismo. Sin embargo, si el Config Server falla, se puede perder la capacidad de direccionar correctamente las consultas.



##  4. Configuraci√≥n inicial de un Replica Set (modo manual)

Supongamos que tenemos 3 servidores MongoDB: `mongo1`, `mongo2`, `mongo3`

### Paso 1: Lanzar instancias con el mismo `--replSet`:

```bash
mongod --replSet rs0 --port 27017 --bind_ip localhost,mongo1
mongod --replSet rs0 --port 27017 --bind_ip localhost,mongo2
mongod --replSet rs0 --port 27017 --bind_ip localhost,mongo3
```

### Paso 2: Conectarse a uno y configurar el Replica Set

```bash
mongo --host mongo1
```

```js
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongo1:27017" },
    { _id: 1, host: "mongo2:27017" },
    { _id: 2, host: "mongo3:27017" }
  ]
})
```

### Paso 3: Ver el estado

```js
rs.status()
```



##  5. Preguntas comunes de examen o entrevista

###  ¬øCu√°ntos nodos necesito para alta disponibilidad?

M√≠nimo **3 nodos** (2 data-bearing + 1 arbiter o 3 full replicas)

###  ¬øPuedo tener dos primaries?

No. Solo puede haber **un primary a la vez**. Si hay partici√≥n de red, los nodos sin qu√≥rum no eligen nuevo primary.

###  ¬øY si uso un arbiter?

El arbiter permite qu√≥rum en n√∫meros pares, pero **no almacena datos** y **no debe usarse si ya hay 3 nodos data-bearing**.

###  ¬øQu√© es la prioridad en los nodos?

Puedes asignar una `priority` para controlar qu√© nodo tiene m√°s probabilidades de ser elegido como primary:

```js
{ _id: 0, host: "mongo1:27017", priority: 2 }
```

###  ¬øQu√© puerto usa MongoDB?

Por defecto: `27017`. Cada instancia debe tener este puerto libre.



##  6. Mantenimiento y operaci√≥n

### Ver el estado en tiempo real:

```bash
mongo --host mongo1
rs.status()
```

### Forzar nueva elecci√≥n:

```js
rs.stepDown()
```

### A√±adir un nuevo nodo:

```js
rs.add("mongo4:27017")
```

### Eliminar un nodo:

```js
rs.remove("mongo2:27017")
```



##  7. Consideraciones importantes

* Los relojes del sistema deben estar **sincronizados** (usar `ntpd` o `chronyd`)
* Los nodos deben tener **visibilidad entre ellos** (resoluci√≥n DNS o `/etc/hosts`)
* No exponer directamente los nodos a internet sin control de accesos y TLS
* Configurar correctamente las **read/write concerns** para ajustar consistencia y disponibilidad


##  8. Conclusi√≥n

Un cl√∫ster de MongoDB basado en Replica Set proporciona una arquitectura sencilla y potente para alta disponibilidad. Sharding permite escalar horizontalmente y soportar peticiones masivas.

Entender estos principios es fundamental para desplegar MongoDB correctamente en Kubernetes, donde cada pod actuar√° como uno de estos nodos.
